"""
Triggers:
- self.trigger("user_text", text)
- self.trigger("user_text_complete", text)
- self.trigger("user_audio_complete", user_bytes)
- self.trigger("recording_start")
- self.trigger("recording_stop")
- self.trigger("vad_start")
- self.trigger("wakeword_start")
- self.trigger("wakeword_end")
- self.trigger("recorded_chunk")
- self.trigger("volume_interrupt")
"""


from RealtimeSTT import AudioToTextRecorderClient
from lingu import cfg, log, Logic, prompt, is_testmode
import numpy as np
import threading
import pyaudio
import base64
import queue
import time
import gc
from difflib import SequenceMatcher
from dataclasses import dataclass
from typing import Deque
from collections import deque

IS_DEBUG = True


main_recorder_model = cfg(
    "listen", "main_recorder_model", default="large-v2")
enable_realtime_transcription = bool(cfg(
    "listen", "enable_realtime_transcription", default=True))
realtime_recorder_model = cfg(
    "listen", "realtime_recorder_model", default="tiny")
realtime_processing_pause = float(cfg(
    "listen", "realtime_processing_pause", default=0.02))
webrtc_sensitivity = int(cfg(
    "listen", "webrtc_sensitivity", default=3))
min_length_of_recording = float(cfg(
    "listen", "min_length_of_recording", default=0.3))
min_gap_between_recordings = float(cfg(
    "listen", "min_gap_between_recordings", default=0.01))
wake_word_timeout = int(cfg(
    "listen", "wake_word_timeout", default=40))
silero_use_onnx = bool(cfg(
    "listen", "enable_realtime_transcription", default=False))
return_to_wakewords_after_silence = int(cfg(
    "listen", "return_to_wakewords_after_silence", default=20))
silero_sensitivity_normal = float(cfg(
    "listen", "silero_sensitivity_normal", default=0.2))
silero_sensitivity_music = float(cfg(
    "listen", "silero_sensitivity_music", default=0.01))
long_term_noise_decay = float(cfg(
    "listen", "long_term_noise_decay", default=0.995))
short_term_noise_decay = float(cfg(
    "listen", "short_term_noise_decay", default=0.9))
allow_speech_interruption = bool(cfg(
    "listen", "allow_speech_interruption", default=True))
sentence_delimiters = cfg(
    "listen", "sentence_delimiters", default='.?!„ÄÇ')
wakeword_backend = cfg(
    "listen", "wakeword_backend", default="pvporcupine") # can be "pvporcupine" or "oww"
openwakeword_model_paths = cfg(
    "listen", "openwakeword_model_paths",
    default=None # for example: "suh_man_tuh.onnx,suh_mahn_thuh.onnx,ling_goo_flex.onnx"
    )
openwakeword_inference_framework = cfg(
    "listen", "openwakeword_inference_framework", default="onnx")
wake_word_buffer_duration = float(cfg(
    "listen", "wake_word_buffer_duration", default=1.0))
wake_words_sensitivity = float(cfg(
    "listen", "wake_words_sensitivity", default=0.35))
silero_deactivity_detection = bool(cfg(
    "listen", "silero_deactivity_detection", default=False))
beam_size = int(cfg(
    "listen", "beam_size", default=5))
beam_size_realtime = int(cfg(
    "listen", "beam_size", default=3))
delay_wakeup_after_assistant_speech = float(cfg(
    "listen", "delay_wakeup_after_assistant_speech", default=0.1)) # compensate for room hall etc
rapid_sentence_end_duration = float(cfg(
    "listen", "rapid_sentence_end_duration", default=0.1))
fast_sentence_end_silence_duration = float(cfg(
    "listen", "fast_sentence_end_silence_duration", default=0.1))

# input_device_name = int(cfg(
#     "listen", "input_device", default=""))
# input_device_index = int(cfg(
#     "listen", "input_device_index", default=-1))


# print("Configured input_device_index", input_device_index)
early_transcription_on_silence = int(cfg(
    "listen", "early_transcription_on_silence", default=0))
use_main_model_for_realtime = bool(cfg(
    "listen", "use_main_model_for_realtime", default=False))
end_of_sentence_detection_pause = float(cfg(
    "listen", "end_of_sentence_detection_pause", default=1.2))
mid_sentence_detection_pause = float(cfg(
    "listen", "mid_sentence_detection_pause", default=2.4))
unknown_sentence_detection_pause = float(cfg(
    "listen", "unknown_sentence_detection_pause", default=1.8))
use_llm_sentence_end_detection = bool(cfg(
    "listen", "end_of_sentence_detection_pause", default=False))
rapid_sentence_end_detection = float(cfg(
    "listen", "rapid_sentence_end_detection", default=0.4))
hard_break_even_on_background_noise = float(cfg(
    "listen", "hard_break_even_on_background_noise", default=3.0))
hard_break_even_on_background_noise_min_texts = int(cfg(
    "listen", "hard_break_even_on_background_noise_min_texts", default=3))
hard_break_even_on_background_noise_min_similarity = float(cfg(
    "listen", "hard_break_even_on_background_noise_min_similarity", default=0.99))
hard_break_even_on_background_noise_min_chars = int(cfg(
    "listen", "hard_break_even_on_background_noise_min_chars", default=15))
wake_words = cfg(
    "listen", "wake_words", default="Jarvis")

compute_type = cfg(
    "listen", "compute_type",
    default = "default"
    )


@dataclass
class TextEntry:
    text: str
    timestamp: float


class ListenLogic(Logic):
    def __init__(self):
        super().__init__()

        self.set_ready = True
        self.recorder = None
        self.last_realtime_text = ""
        self.start_listen_event = threading.Event()
        self.speech_ready_event = threading.Event()
        self.listen_ready_event = threading.Event()
        self.text_queue = queue.Queue()
        self.recorder_active = True
        self.long_term_noise_level = 0.0
        self.current_noise_level = 0.0
        self.prob_sentence_end_start_time = None
        self.text_history: Deque[TextEntry] = Deque()
        self.max_history_age = 1.0  # 1 second
        self.speech_finished_cache = {}
        self.prev_text = ""
        self.post_speech_silence_duration = 0
        self.text_time_deque = deque()

        self.add_listener("playback_start", "music", self._on_playback_start)
        self.add_listener("playback_stop", "music", self._on_playback_stop)
        self.add_listener("speech_ready", "speech", self._on_speech_ready)
        self.add_listener(
            "audio_stream_stop",
            "speech",
            self._on_audio_stream_stop)
        self.add_listener(
            "symbol_mouse_enter",
            "*",
            self._on_wake_up)
        self.add_listener(
            "stop_recorder",
            "speech",
            self._on_stop_recorder)
        self.add_listener(
            "client_connected",
            "server",
            self._disable_recording)
        self.add_listener(
            "client_disconnected",
            "server",
            self._enable_recording)
        self.add_listener(
            "client_chunk_received",
            "server",
            self._client_chunk_received)

        self.worker_thread = threading.Thread(target=self.process_queue, daemon=True)
        self.worker_thread.start()

    def _client_chunk_received(self, data):
        self.recorder.feed_audio(data)

    def _disable_recording(self):
        self.state.set_text("üåê")
        log.inf("  [listen] client connected, disabling server microphone")
        self.recorder.set_microphone(False)

    def _enable_recording(self):
        self.state.set_text("")
        log.inf("  [listen] client disconnected, enabling server microphone")
        self.recorder.set_microphone(True)

    def sleep(self):
        self.start_listen_event.clear()
        self.recorder.abort()
        self.recorder.stop()
        self.state.set_active(False)
        self.state.set_disabled(True)

    def wakeup(self):
        self.recorder.clear_audio_queue()
        self.recorder.set_parameter("recording_stop_time", 0)
        self.state.set_disabled(False)
        self.recorder.wakeup()
        self.start_listen_event.set()
        self.recorder.set_parameter("listen_start",time.time())

    def set_lang_shortcut(self, lang_shortcut):
        log.inf(f"  [listen] setting language shortcut to {lang_shortcut}")
        self.state.language = lang_shortcut
        self.state.save()
        self.destroy_recorder()
        self.create_recorder()

    def _on_stop_recorder(self):
        log.inf("  [listen] stop recorder")
        self.sleep()

    def delayed_wakeup(self):
        log.dbg(f"Delaying wakeup for {delay_wakeup_after_assistant_speech} seconds")
        time.sleep(delay_wakeup_after_assistant_speech)
        self.wakeup()

    def _on_audio_stream_stop(self):
        log.dbg("  [listen] audio stream stop")
        threading.Thread(
            target=self.delayed_wakeup
        ).start()

    def _on_speech_ready(self):
        self.speech_ready_event.set()

    def _on_wake_up(self):
        self.recorder.set_parameter("listen_start",time.time())        

    def _realtime_transcription(self, text: str):
        text = text.strip()
        current_time = time.time()

        # Remove old entries
        self.text_history = Deque([entry for entry in self.text_history if current_time - entry.timestamp <= self.max_history_age])

        # Add new entry
        self.text_history.append(TextEntry(text, current_time))

        self.trigger("user_text", text)
        self.text_queue.put(text)


    def _final_text(self, text):
        log.inf(f"  [listen] speech end detected, post_speech_silence_duration: {self.post_speech_silence_duration}")
        prompt.reset()

        self.trigger("before_user_text_complete", text)
        self.trigger("user_text_complete", text)

        last_transcription_bytes = self.recorder.get_parameter("last_transcription_bytes_b64")
        decoded_bytes = base64.b64decode(last_transcription_bytes)

        # # Step 2: Reconstruct the np.int16 array from the decoded bytes
        audio_array = np.frombuffer(decoded_bytes, dtype=np.int16)

        # Step 3: (Optional) If the original data was normalized, convert to np.float32 and normalize
        INT16_MAX_ABS_VALUE = 32768.0
        normalized_audio = audio_array.astype(np.float32) / INT16_MAX_ABS_VALUE        
        self.trigger("user_audio_complete", normalized_audio)

        if hasattr(self, 'on_final_text'):
            self.on_final_text(text)

        if IS_DEBUG: print(f"SENTENCE: post_speech_silence_duration: {self.post_speech_silence_duration}")
        print(f"_final_text setting post_speech_silence_duration to {unknown_sentence_detection_pause}")
        self.set_post_speech_silence_duration(unknown_sentence_detection_pause)
        text = self.preprocess_text(text)
        text = text.rstrip()
        self.text_time_deque.clear()
        if text.endswith("..."):
            text = text[:-2]

        self.prev_text = ""

    def listen(self):
        self.start_listen_event.set()

    def init_finished(self):
        self.create_recorder()

    def create_recorder(self):
        self.recorder_active = True

        self.recording_worker_thread = threading.Thread(
            target=self._recording_worker
        )
        self.recording_worker_thread.daemon = True
        self.recording_worker_thread.start()

        self.listen()

    def destroy_recorder(self):
        if self.recording_worker_thread:
            self.recorder_active = False
            self.recorder.abort()
            self.recording_worker_thread.join()
            self.recording_worker_thread = None

    def _on_playback_start(self):
        self.recorder.set_parameter("silero_sensitivity",silero_sensitivity_music)
        self.recorder.set_parameter("wake_word_activation_delay", 0)

    def _on_playback_stop(self):
        self.recorder.set_parameter("silero_sensitivity",silero_sensitivity_normal)
        self.recorder.set_parameter("wake_word_activation_delay", return_to_wakewords_after_silence)

    def toggle_mute(self):
        self.state.is_muted = not self.state.is_muted

        if self.state.is_muted:
            self.sleep()
        else:
            self.wakeup()

    def set_post_speech_silence_duration(self, post_speech_silence_duration):
        self.recorder.set_parameter("post_speech_silence_duration", post_speech_silence_duration)
        self.post_speech_silence_duration = post_speech_silence_duration

    def _recording_start(self):
        log.dbg("  [listen] recording start")
        print(f"_recording_start setting post_speech_silence_duration to {self.state.end_of_speech_silence} (self.state.end_of_speech_silence)")
        self.set_post_speech_silence_duration(self.state.end_of_speech_silence)
        self.last_realtime_text = ""
        self.state.set_active(True)
        self.trigger("recording_start")

    def _recording_stop(self):
        log.dbg("  [listen] recording stop")
        self.state.set_active(False)
        self.start_listen_event.clear()
        self.state.set_disabled(True)
        self.trigger("recording_stop")
        log.dbg("  [listen] recording stop triggered")

    def _vad_start(self):
        self.trigger("vad_start")

    def _wakeword_start(self):
        log.dbg(f"  [listen] listening to wakeword")
        self.trigger("wakeword_start")

    def _wakeword_end(self):
        log.dbg(f"  [listen] stopped listening to wakeword")
        self.trigger("wakeword_end")

    def _transcription_start(self):
        if hasattr(self, 'on_transcription_start'):
            self.on_transcription_start()

    def set_start_timeout(self, value):
        log.inf(f"  [listen] setting start timeout to {value:.1f} seconds")
        self.recorder.set_parameter("wake_word_activation_delay", value)
        self.state.wake_word_activation_delay = value
        self.state.save()

    def set_end_timeout(self, value):
        log.inf(f"  [listen] setting end timeout to {value:.1f} seconds")
        print(f"set_end_timeout setting post_speech_silence_duration to {value} (value)")
        self.set_post_speech_silence_duration(value)
        self.state.end_of_speech_silence = value
        self.state.save()

    def get_levels(self, data, long_term_noise_level, current_noise_level):

        pegel = np.abs(np.frombuffer(data, dtype=np.int16)).mean()

        long_term_noise_level = \
            (long_term_noise_level * long_term_noise_decay
                + pegel * (1.0 - long_term_noise_decay))

        current_noise_level = \
            (current_noise_level * short_term_noise_decay
                + pegel * (1.0 - short_term_noise_decay))

        return pegel, long_term_noise_level, current_noise_level

    def _recorded_chunk(self, chunk):
        pegel, self.long_term_noise_level, self.current_noise_level = \
            self.get_levels(
                chunk,
                self.long_term_noise_level,
                self.current_noise_level)

        if self.state.interrupt_thresh > 0 and allow_speech_interruption:
            if pegel > self.state.interrupt_thresh:
                self.trigger("volume_interrupt")

        self.state.pegel = pegel
        self.trigger("recorded_chunk")

    def get_input_device_index_by_name(self, device_name):
        try:
            p = pyaudio.PyAudio()
            device_count = p.get_device_count()
            matching_devices = []
            for i in range(device_count):
                device_info = p.get_device_info_by_index(i)
                device_name_lower = device_info["name"].lower()
                search_name_lower = device_name.lower()
                if search_name_lower in device_name_lower and device_info["maxInputChannels"] > 0:
                    matching_devices.append((i, device_info["name"]))
            if matching_devices:
                # If multiple devices match, select the first one
                selected_index, selected_name = matching_devices[0]
                print(f"  [listen] Matched input device: '{selected_name}' with index {selected_index}")
                return selected_index
            else:
                return None
        except ImportError:
            print("pyaudio is not installed. Cannot find input device by name.")
            return None

    def _recording_worker(self):

        lang = self.state.language
        if lang == "Auto":
            lang = ""

        # Prioritize input_device_name over input_device_index
        input_device_name_cfg = cfg("listen", "input_device", default="")
        input_device_name = input_device_name_cfg.strip()
        input_device_index_cfg = int(cfg("listen", "input_device_index", default=-1))
        input_device_index = None  # Default to None

        log.inf(f"  [listen] input_device_name: '{input_device_name}', input_device_index: {input_device_index_cfg}")

        if input_device_name:
            # Attempt to find the device index by partial name
            print(f"  [listen] Attempting to find device index for device name containing: '{input_device_name}'")
            input_device_index = self.get_input_device_index_by_name(input_device_name)
            if input_device_index is not None:
                print(f"  [listen] Found input device containing '{input_device_name}' with index {input_device_index}")
            else:
                print(f"  [listen] Input device containing '{input_device_name}' not found. Using 'input_device_index' from config if valid.")
                if input_device_index_cfg != -1:
                    input_device_index = input_device_index_cfg
        else:
            # Use input_device_index from config if valid
            if input_device_index_cfg != -1:
                input_device_index = input_device_index_cfg

        if input_device_index == -1:
          input_device_index = None

        print(f"  [listen] Using input_device_index: {input_device_index}")

        # recorder construction parameters dictionary
        recorder_params = {
            'model': main_recorder_model,
            'compute_type' : compute_type,
            'input_device_index': input_device_index,
            'language': lang,
            'wake_words': wake_words,
            'wake_words_sensitivity': wake_words_sensitivity,
            'spinner': False,
            'silero_sensitivity': silero_sensitivity_normal,
            'silero_use_onnx': silero_use_onnx,
            'webrtc_sensitivity': webrtc_sensitivity,
            'on_recording_start': self._recording_start,
            'on_recording_stop': self._recording_stop,
            'on_vad_detect_start': self._vad_start,
            'on_wakeword_detection_start': self._wakeword_start,
            'on_wakeword_detection_end': self._wakeword_end,
            'on_transcription_start': self._transcription_start,
            'post_speech_silence_duration': self.state.end_of_speech_silence,
            'min_length_of_recording': min_length_of_recording,
            'min_gap_between_recordings': min_gap_between_recordings,
            'wake_word_timeout': wake_word_timeout,
            'wake_word_activation_delay': self.state.wake_word_activation_delay,
            'wakeword_backend': wakeword_backend,
            'openwakeword_model_paths': openwakeword_model_paths,
            'openwakeword_inference_framework': openwakeword_inference_framework,
            'wake_word_buffer_duration': wake_word_buffer_duration,
            'enable_realtime_transcription': enable_realtime_transcription,
            'realtime_processing_pause': realtime_processing_pause,
            'realtime_model_type': realtime_recorder_model,
            'use_main_model_for_realtime' : use_main_model_for_realtime,
            'on_realtime_transcription_update': self._realtime_transcription,
            'on_recorded_chunk': self._recorded_chunk,
            'silero_deactivity_detection': silero_deactivity_detection,
            'beam_size': beam_size,
            'beam_size_realtime': beam_size_realtime,
            'initial_prompt': (
                "End incomplete sentences with ellipses.\n"
                "Examples:\n"
                "Complete: The sky is blue.\n"
                "Incomplete: When the sky...\n"
                "Complete: She walked home.\n"
                "Incomplete: Because he...\n"
            ),
            'debug_mode': True,
            'use_extended_logging': True,
        }

        # log the parameters
        log.dbg("  [listen] creating recorder with parameters: "
                f"{recorder_params}")

        print("  [listen] creating recorder with parameters: "
                f"{recorder_params}")

        # Initialize the recorder with the unpacked parameters
        self.recorder = AudioToTextRecorderClient(**recorder_params)

        self.post_speech_silence_duration = self.recorder.get_parameter("post_speech_silence_duration")

        self.speech_ready_event.wait()
        self.start_listen_event.wait()

        self.ready()
        self.server_ready_event.wait()
        if not is_testmode():
            log.inf("  [listen] start listening")
        else:
            log.inf("  [listen] test mode, listening deactivated")

        def final_text(text):
            if text:
                log.inf("  [listen] final text arrived")
                self._final_text(text)

        self.listen_ready_event.set()
        while (self.recorder_active):
            self.start_listen_event.wait()

            self.start_listen_event.clear()

            if not is_testmode():
                self.recorder.text(final_text)

        self.start_listen_event.clear()

        self.recorder.shutdown()
        del self.recorder
        self.recorder = None
        gc.collect()

    def preprocess_text(self, text):
        # Remove leading whitespaces
        text = text.lstrip()

        #  Remove starting ellipses if present
        if text.startswith("..."):
            text = text[3:]

        # Remove any leading whitespaces again after ellipses removal
        text = text.lstrip()

        # Uppercase the first letter
        if text:
            text = text[0].upper() + text[1:]
        
        return text

    def is_speech_finished(self, text):
         #return False
        # Check if the result is already in the cache
        if text in self.speech_finished_cache:
            if IS_DEBUG:
                print(f"Cache hit for: '{text}'")
            return self.speech_finished_cache[text]
        
        user_prompt = (
            "Please reply with only 'c' if the following text is a complete thought (a sentence that stands on its own), "
            "or 'i' if it is not finished. Do not include any additional text in your reply. "
            "Consider a full sentence to have a clear subject, verb, and predicate or express a complete idea. "
            "Examples:\n"
            "- 'The sky is blue.' is complete (reply 'c').\n"
            "- 'When the sky' is incomplete (reply 'i').\n"
            "- 'She walked home.' is complete (reply 'c').\n"
            "- 'Because he' is incomplete (reply 'i').\n"
            f"\nText: {text}"
        )

        response = self.llm(
            messages=[{"role": "user", "content": user_prompt}],
            max_tokens=1,
            temperature=0.0,  # Set temperature to 0 for deterministic output
        )

        reply = response.choices[0].message.content.strip().lower()
        if IS_DEBUG:
            print(f"t:'{reply}'", end="", flush=True)

        result = reply == 'c'

        # Cache the result
        self.speech_finished_cache[text] = result

        return result

    def process_queue(self):
        def ends_with_ellipsis(text: str):
            if text.endswith("..."):
                return True
            if len(text) > 1 and text[:-1].endswith("..."):
                return True
            return False

        def sentence_end(text: str):
            sentence_end_marks = ['.', '!', '?', '„ÄÇ']
            if text and text[-1] in sentence_end_marks:
                return True
            return False

        # Initialize a deque to store texts with their timestamps
        while True:
            try:
                text = self.text_queue.get(timeout=1)  # Wait for text or timeout after 1 second
            except queue.Empty:
                continue  # No text to process, continue looping

            if text is None:
                # Sentinel value to indicate thread should exit
                break

            # self.post_speech_silence_duration = self.recorder.get_parameter("post_speech_silence_duration")
            log.inf(f"  [listen] process_queue realtime, post_speech_silence_duration: {self.post_speech_silence_duration}")

            text = self.preprocess_text(text)
            current_time = time.time()

            if ends_with_ellipsis(text):
                if not self.post_speech_silence_duration == mid_sentence_detection_pause:
                    self.set_post_speech_silence_duration(mid_sentence_detection_pause)
                    if IS_DEBUG: print(f"RT: post_speech_silence_duration for {text} (...): {self.post_speech_silence_duration}")
            elif sentence_end(text) and sentence_end(self.prev_text) and not ends_with_ellipsis(self.prev_text):
                if not self.post_speech_silence_duration == end_of_sentence_detection_pause:
                    self.set_post_speech_silence_duration(end_of_sentence_detection_pause)
                    if IS_DEBUG: print(f"RT: post_speech_silence_duration for {text} (.!?): {self.post_speech_silence_duration}")
            else:
                if not self.post_speech_silence_duration == unknown_sentence_detection_pause:
                    self.set_post_speech_silence_duration(unknown_sentence_detection_pause)
                    if IS_DEBUG: print(f"RT: post_speech_silence_duration for {text} (???): {self.post_speech_silence_duration}")

            self.prev_text = text
            
            import string
            transtext = text.translate(str.maketrans('', '', string.punctuation))
            
            if use_llm_sentence_end_detection and self.is_speech_finished(transtext):
                if not self.post_speech_silence_duration == rapid_sentence_end_detection:
                    self.set_post_speech_silence_duration(rapid_sentence_end_detection)
                    if IS_DEBUG: print(f"RT: {transtext} post_speech_silence_duration: {self.post_speech_silence_duration}")

            # Append the new text with its timestamp
            self.text_time_deque.append((current_time, text))

            # Remove texts older than hard_break_even_on_background_noise seconds
            while self.text_time_deque and self.text_time_deque[0][0] < current_time - hard_break_even_on_background_noise:
                self.text_time_deque.popleft()

            # Check if at least hard_break_even_on_background_noise_min_texts texts have arrived within the last hard_break_even_on_background_noise seconds
            if len(self.text_time_deque) >= hard_break_even_on_background_noise_min_texts:
                texts = [t[1] for t in self.text_time_deque]
                first_text = texts[0]
                last_text = texts[-1]

                # Compute the similarity ratio between the first and last texts
                similarity = SequenceMatcher(None, first_text, last_text).ratio()

                if similarity > hard_break_even_on_background_noise_min_similarity and len(first_text) > hard_break_even_on_background_noise_min_chars:
                    self.recorder.stop()

            # Mark the task as done
            self.text_queue.task_done()


if __name__ == '__main__':
    logic = ListenLogic()
